To me, the most important topic discussed in the Nature article isn't that we should abandon the use of p-values, confidence intervals, and language such as "statistically significant." While there definitely appear to be serious abuses of interpreting these statistical measures in the results/conclusions sections of numerous papers, I think there is a much larger issue at hand. Instead, I believe that the primary focus of scientists/statisticians should be on the need for conducting and reporting science in a way that is more open, experimental design driven, and results blind. For this reason, I read the Amrhein, Trafimow, and Greenland paper "Inferential Statistics as Descriptive Statistics: There Is No Replication Crisis if We Donâ€™t Expect Replication."

The paper discusses the ongoing "replication crisis" in science - that many papers with very similar methods can produce different results. There are at least two clear causes for the crisis: 1) troublesome interpretations resulting from the use of hypothesis testing and p-values and 2) methods/data/designs that are not reported well (or at all) and/or are difficult to follow. The authors discuss how the first problem is generally unavoidable due to the nature of trying to apply simplified statistical models to complex systems. My favorite quote from the paper is when the authors compare using p-values to drinking alcohol - fine to use in moderation but can easily be abused and can potentially be dangerous (drinking and driving). To address problem 2, the authors suggest possibly implementing "results blind" publishing - that is, a journal will review the methods and experimental design of a paper and determine if the experiment is worth publishing before data has been collected and reported. I think this is a great idea. If a researcher is focused on asking important questions using a design/method that is appropriate, then the results will be useful regardless of the outcome. Furthermore, if the researcher focuses on explaining their methods and experiments in a thorough and understandable way, and includes their data/code, then it will be easier for other scientists to repeat their experiments or perform meta-analysis. The paper ends by asking "how can we be confident in knowing something is true?" with the authors suggesting that we must make judgements based on cumulative knowledge generated through continued studies (rather than single studies claiming to provide decisive results). Focusing primarily on developing and explaining your methods, collecting good data, and making your analysis more transparent rather than focusing solely on whether you have a "significant" p-value will better improve the cumulative knowledge base and thus improve our confidence in scientific findings.